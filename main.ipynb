{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Setup - Imports, Device Config, Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "# from model import RW_NN\n",
    "from utils import load_data, generate_batches, accuracy, AverageMeter\n",
    "import torch.nn as nn\n",
    "\n",
    "from cdlib import algorithms\n",
    "from networkx.algorithms.community import girvan_newman\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import networkx as nx\n",
    "from preprocessing import preprocess_graph\n",
    "import vgae_model\n",
    "from types import SimpleNamespace\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import random\n",
    "\n",
    "# density clustering\n",
    "from sklearn.cluster import DBSCAN\n",
    "# hierarchical clustering\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "# spectral clustering\n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "from types import SimpleNamespace\n",
    "args = SimpleNamespace(\n",
    "    dataset='REDDIT-BINARY',\n",
    "    use_node_labels=False,\n",
    "    lr=1e-2,\n",
    "    dropout=0.2,\n",
    "    batch_size=64,\n",
    "    epochs=500,\n",
    "    hidden_graphs=16,\n",
    "    size_hidden_graphs=5,\n",
    "    hidden_dim=4,\n",
    "    penultimate_dim=5,\n",
    "    max_step=1,\n",
    "    normalize=False,\n",
    "    num_samples=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgae_args = SimpleNamespace(\n",
    "    ### CONFIGS ###\n",
    "    model = 'VGAE',\n",
    "    input_dim = args.size_hidden_graphs, \n",
    "    hidden1_dim = 4,\n",
    "    hidden2_dim = 3,\n",
    "    use_feature = True,\n",
    "    num_epoch = 100,\n",
    "    learning_rate = 0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_lst, features_lst, class_labels = load_data(args.dataset, args.use_node_labels)\n",
    "\n",
    "N = len(adj_lst)\n",
    "features_dim = features_lst[0].shape[1]\n",
    "\n",
    "enc = LabelEncoder()\n",
    "class_labels = enc.fit_transform(class_labels)\n",
    "n_classes = np.unique(class_labels).size\n",
    "y = [np.array(class_labels[i]) for i in range(class_labels.size)]\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "it = 0\n",
    "accs = list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RWNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGAE Arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distance Based Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RWNN Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE MODEL\n",
    "class RW_NN(nn.Module):\n",
    "    # new hyperparameters\n",
    "    # max_num_children\n",
    "    def __init__(self, input_dim, max_step, hidden_graphs, size_hidden_graphs, hidden_dim, penultimate_dim, normalize, n_classes, dropout, device):\n",
    "        super(RW_NN, self).__init__()\n",
    "        self.max_step = max_step\n",
    "        self.hidden_graphs = hidden_graphs\n",
    "        self.size_hidden_graphs = size_hidden_graphs\n",
    "        self.normalize = normalize\n",
    "        self.device = device\n",
    "\n",
    "        self.adj_hidden = Parameter(torch.FloatTensor(hidden_graphs, (size_hidden_graphs*(size_hidden_graphs-1))//2))\n",
    "        self.adj_hidden_tree = None\n",
    "        self.adj_hidden_tree_norm = None\n",
    "        self.features_hidden = Parameter(torch.FloatTensor(hidden_graphs, size_hidden_graphs, hidden_dim))\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.features_hidden_tree = None\n",
    "        self.input_dim = input_dim\n",
    "        self.penultimate_dim = penultimate_dim\n",
    "\n",
    "        self.fc = torch.nn.Linear(input_dim, hidden_dim)\n",
    "        self.bn = nn.BatchNorm1d(hidden_graphs*max_step)\n",
    "        self.fc1 = torch.nn.Linear(hidden_graphs*max_step, penultimate_dim)\n",
    "        self.fc2 = torch.nn.Linear(penultimate_dim, n_classes)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.init_weights()\n",
    "        \n",
    "        self.metagraph = None\n",
    "        self.cluster_labels = None\n",
    "        self.vgae_graphs = None\n",
    "\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.adj_hidden.data.uniform_(-1, 1)\n",
    "        self.features_hidden.data.uniform_(0, 1)\n",
    "\n",
    "        \n",
    "    def forward(self, adj, features, graph_indicator, y=None):\n",
    "        if self.training:\n",
    "            unique, counts = torch.unique(graph_indicator, return_counts=True)\n",
    "            n_graphs = unique.size(0)\n",
    "            n_nodes = features.size(0)\n",
    "\n",
    "            if self.normalize:\n",
    "                norm = counts.unsqueeze(1).repeat(1, self.hidden_graphs)\n",
    "            adj_hidden_norm = torch.zeros(self.hidden_graphs, self.size_hidden_graphs, self.size_hidden_graphs).to(self.device)\n",
    "            idx = torch.triu_indices(self.size_hidden_graphs, self.size_hidden_graphs, 1)\n",
    "            adj_hidden_norm[:,idx[0],idx[1]] = self.relu(self.adj_hidden)\n",
    "            adj_hidden_norm = adj_hidden_norm + torch.transpose(adj_hidden_norm, 1, 2)\n",
    "            adj_hidden_norm[adj_hidden_norm > 0] = 1\n",
    "\n",
    "            x = self.sigmoid(self.fc(features))\n",
    "            z = self.features_hidden\n",
    "            zx = torch.einsum(\"abc,dc->abd\", (z, x))\n",
    "            \n",
    "            out = list()\n",
    "            for i in range(self.max_step):\n",
    "                if i == 0:\n",
    "                    eye = torch.eye(self.size_hidden_graphs, device=self.device)\n",
    "                    eye = eye.repeat(self.hidden_graphs, 1, 1)              \n",
    "                    o = torch.einsum(\"abc,acd->abd\", (eye, z))\n",
    "                    t = torch.einsum(\"abc,dc->abd\", (o, x))\n",
    "                else:\n",
    "                    x = torch.spmm(adj, x)\n",
    "                    z = torch.einsum(\"abc,acd->abd\", (adj_hidden_norm, z))\n",
    "                    t = torch.einsum(\"abc,dc->abd\", (z, x))\n",
    "                t = self.dropout(t)\n",
    "                t = torch.mul(zx, t)\n",
    "                t = torch.zeros(t.size(0), t.size(1), n_graphs, device=self.device).index_add_(2, graph_indicator, t)\n",
    "                t = torch.sum(t, dim=1)\n",
    "                t = torch.transpose(t, 0, 1)\n",
    "                if self.normalize:\n",
    "                    t /= norm\n",
    "                out.append(t)\n",
    "                \n",
    "            out = torch.cat(out, dim=1)\n",
    "            out = self.bn(out)\n",
    "            out = self.relu(self.fc1(out))\n",
    "            out = self.dropout(out)\n",
    "            out = self.fc2(out)\n",
    "            return F.log_softmax(out, dim=1)\n",
    "        else:\n",
    "            num_samples = args.num_samples\n",
    "            adj_hidden_vgae, vgae_features = self.run_vgae(num_samples)\n",
    "            unique, counts = torch.unique(graph_indicator, return_counts=True)\n",
    "            n_graphs = unique.size(0)\n",
    "            n_nodes = features.size(0)\n",
    "\n",
    "            if self.normalize:\n",
    "                norm = counts.unsqueeze(1).repeat(1, self.hidden_graphs)\n",
    "            adj_hidden_norm = torch.zeros(self.hidden_graphs, self.size_hidden_graphs, self.size_hidden_graphs).to(self.device)\n",
    "            idx = torch.triu_indices(self.size_hidden_graphs, self.size_hidden_graphs, 1)\n",
    "            adj_hidden_norm[:,idx[0],idx[1]] = self.relu(adj_hidden_vgae)\n",
    "            adj_hidden_norm = adj_hidden_norm + torch.transpose(adj_hidden_norm, 1, 2)\n",
    "            x = self.sigmoid(self.fc(features))\n",
    "            z = self.features_hidden\n",
    "            zx = torch.einsum(\"abc,dc->abd\", (z, x))\n",
    "            \n",
    "            out = list()\n",
    "            for i in range(self.max_step):\n",
    "                if i == 0:\n",
    "                    eye = torch.eye(self.size_hidden_graphs, device=self.device)\n",
    "                    eye = eye.repeat(self.hidden_graphs, 1, 1)              \n",
    "                    o = torch.einsum(\"abc,acd->abd\", (eye, z))\n",
    "                    t = torch.einsum(\"abc,dc->abd\", (o, x))\n",
    "                else:\n",
    "                    x = torch.spmm(adj, x)\n",
    "                    z = torch.einsum(\"abc,acd->abd\", (adj_hidden_norm, z))\n",
    "                    t = torch.einsum(\"abc,dc->abd\", (z, x))\n",
    "                t = self.dropout(t)\n",
    "                t = torch.mul(zx, t)\n",
    "                t = torch.zeros(t.size(0), t.size(1), n_graphs, device=self.device).index_add_(2, graph_indicator, t)\n",
    "                t = torch.sum(t, dim=1)\n",
    "                t = torch.transpose(t, 0, 1)\n",
    "                if self.normalize:\n",
    "                    t /= norm\n",
    "                out.append(t)\n",
    "                \n",
    "            out = torch.cat(out, dim=1)\n",
    "            out = self.bn(out)\n",
    "            out = self.relu(self.fc1(out))\n",
    "            out = self.dropout(out)\n",
    "            out = self.fc2(out)\n",
    "            return F.log_softmax(out, dim=1)\n",
    "        \n",
    "\n",
    "    \n",
    "    def get_hidden_graphs_adjacency_list(self):\n",
    "        \"\"\"Converts the upper-triangular adjacency data to adjacency lists.\"\"\"\n",
    "        adj_hidden_norm = torch.zeros(self.hidden_graphs, self.size_hidden_graphs, self.size_hidden_graphs).to(self.device)\n",
    "        idx = torch.triu_indices(self.size_hidden_graphs, self.size_hidden_graphs, 1)\n",
    "        adj_hidden_norm[:, idx[0], idx[1]] = self.relu(self.adj_hidden)\n",
    "        adj_hidden_norm = adj_hidden_norm + torch.transpose(adj_hidden_norm, 1, 2)\n",
    "        \n",
    "        # Convert each hidden graph's adjacency matrix to an adjacency list\n",
    "        adjacency_lists = []\n",
    "        for i in range(self.hidden_graphs):\n",
    "            adj_list = {}\n",
    "            adj_matrix = adj_hidden_norm[i].detach().cpu().numpy()  # Move to CPU for easy processing\n",
    "            for row in range(adj_matrix.shape[0]):\n",
    "                adj_list[row] = list(np.where(adj_matrix[row] > 0)[0])  # Find connected nodes\n",
    "            adjacency_lists.append(adj_list)\n",
    "        \n",
    "        return adjacency_lists\n",
    "    \n",
    "\n",
    "\n",
    "# VGAE METHODS\n",
    "    def sparse_to_tuple(self, sparse_mx):\n",
    "        if not sp.isspmatrix_coo(sparse_mx):\n",
    "            sparse_mx = sparse_mx.tocoo()\n",
    "        coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n",
    "        values = sparse_mx.data\n",
    "        shape = sparse_mx.shape\n",
    "        return coords, values, shape\n",
    "    def mask_test_edges(self, adj):\n",
    "        adj = adj - sp.dia_matrix((adj.diagonal()[np.newaxis, :], [0]), shape=adj.shape)\n",
    "        adj.eliminate_zeros()\n",
    "\n",
    "        adj_triu = sp.triu(adj)\n",
    "        adj_tuple = self.sparse_to_tuple(adj_triu)\n",
    "        edges = adj_tuple[0]\n",
    "\n",
    "        num_edges = edges.shape[0]\n",
    "        if num_edges < 10:\n",
    "            # print(\"Not enough edges for a split. Using all edges for training.\")\n",
    "            return edges, [], []  # Return all edges for training, no validation/test\n",
    "\n",
    "        num_test = int(np.floor(num_edges / 10.))\n",
    "        num_val = int(np.floor(num_edges / 20.))\n",
    "\n",
    "        all_edge_idx = list(range(num_edges))\n",
    "        np.random.shuffle(all_edge_idx)\n",
    "        val_edge_idx = all_edge_idx[:num_val]\n",
    "        test_edge_idx = all_edge_idx[num_val:(num_val + num_test)]\n",
    "\n",
    "        test_edges = edges[test_edge_idx] if test_edge_idx else []\n",
    "        val_edges = edges[val_edge_idx] if val_edge_idx else []\n",
    "        if test_edge_idx or val_edge_idx:\n",
    "            train_edges = np.delete(edges, np.hstack([test_edge_idx, val_edge_idx]), axis=0)\n",
    "        else:\n",
    "            train_edges = edges\n",
    "\n",
    "        return train_edges, val_edges, test_edges\n",
    "    def get_acc(self, adj_rec, adj_label):\n",
    "        labels_all = adj_label.to_dense().view(-1).long()\n",
    "        preds_all = (adj_rec > 0.5).view(-1).long()\n",
    "        accuracy = (preds_all == labels_all).sum().float() / labels_all.size(0)\n",
    "        return accuracy\n",
    "    \n",
    "    def genMetaGraphs(self, graph_dist_adj_matrix):\n",
    "        G = nx.from_numpy_array(graph_dist_adj_matrix)\n",
    "\n",
    "        pos = nx.spring_layout(G)\n",
    "\n",
    "        nx.draw_networkx_nodes(G, pos)\n",
    "\n",
    "        labels = nx.get_edge_attributes(G, 'weight')\n",
    "        nx.draw_networkx_edges(G, pos)\n",
    "        nx.draw_networkx_edge_labels(G, pos, edge_labels=labels)\n",
    "\n",
    "        plt.show()\n",
    "    # NUM_SAMPLES = NUM_CLUSTERS\n",
    "    def getSamples(self, num_samples):\n",
    "        adjacency_lists = self.get_hidden_graphs_adjacency_list()\n",
    "        # for graph_id, adj_list in enumerate(adjacency_lists):\n",
    "            # if len(adj_list) != 5:\n",
    "            #     print(\"FIRST ONE NOT GOOD\")\n",
    "            #     raise ValueError(f\"Graph {graph_id} does not have exactly 5 nodes. It has {len(adj_list)} nodes.\")\n",
    "        graphs = []\n",
    "\n",
    "        # Draw the adjacency list as a graph using NetworkX and Matplotlib\n",
    "        for i, adj_list in enumerate(adjacency_lists):\n",
    "            G = nx.Graph()  # Create a new graph\n",
    "            # Add edges to the graph based on the adjacency list\n",
    "            for node, neighbors in adj_list.items():\n",
    "                G.add_node(node)\n",
    "                for neighbor in neighbors:\n",
    "                    G.add_edge(node, neighbor)\n",
    "\n",
    "            # if len(G.nodes) != 5:\n",
    "            #     raise ValueError(f\"Graph {i} does not have exactly 5 nodes. It has {len(G.nodes)} nodes.\")\n",
    "\n",
    "            # for j, graph in enumerate(graphs):\n",
    "            #     if nx.is_isomorphic(graph, G):\n",
    "            #         print(\"Graph\", i, \"is isomorphic to previous graph\", j)\n",
    "            graphs.append(G)\n",
    "\n",
    "        #Edit distance\n",
    "        graph_dist_adj_matrix = np.zeros(shape=(len(graphs),len(graphs)))\n",
    "        \n",
    "        for i, graph_1 in enumerate(graphs):\n",
    "            for j, graph_2 in enumerate(graphs):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                graph_edit_distance = nx.graph_edit_distance(graph_1, graph_2)\n",
    "                graph_dist_adj_matrix[i, j] = graph_edit_distance\n",
    "        graph_dist_adj_matrix_rounded = np.round(graph_dist_adj_matrix, 3)\n",
    "\n",
    "        G = nx.from_numpy_array(graph_dist_adj_matrix_rounded)\n",
    "        self.metagraph = G\n",
    "\n",
    "        # sampling\n",
    "        \n",
    "        node_features = graph_dist_adj_matrix\n",
    "\n",
    "        # apply PCA for dimensionality reduction\n",
    "        pca = PCA(n_components=2)  # Reduce to 2 components for visualization\n",
    "        reduced_features = pca.fit_transform(node_features)\n",
    "        \n",
    "        # apply clustering\n",
    "        num_clusters = num_samples\n",
    "        kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "        # dbscan = DBSCAN(eps=2, min_samples=2)\n",
    "        # node_features = node_features.reshape(-1, 1)\n",
    "        k_spectral = SpectralClustering(n_clusters=num_clusters, affinity='nearest_neighbors', n_neighbors = 10, random_state=42)\n",
    "        hierarchical = AgglomerativeClustering(n_clusters=num_clusters, linkage='ward')\n",
    "        gamma_spectral = SpectralClustering(n_clusters=num_clusters, affinity='rbf', gamma = 2.0, random_state=42)\n",
    "        # com = algorithms.walktrap(G)\n",
    "        # com = girvan_newman(G)\n",
    "        # communities = list(com)\n",
    "        # print(communities)\n",
    "        # node_to_cluster_mapping = {}\n",
    "        # for cluster_id, community in enumerate(communities):\n",
    "        #     for node in community:\n",
    "        #         node_to_cluster_mapping[node] = cluster_id\n",
    "\n",
    "        # Extract cluster labels\n",
    "        #cluster_labels = [node_to_cluster_mapping[node] for node in range(len(G.nodes()))]\n",
    "\n",
    "        # print(\"Cluster Labels:\", cluster_labels)\n",
    "        cluster_labels = hierarchical.fit_predict(reduced_features)# gamma_spectral.fit_predict(reduced_features)\n",
    "        self.cluster_labels = cluster_labels\n",
    "        \n",
    "        # Step 1: Group graph indices by cluster labels\n",
    "        cluster_to_graphs = {i: [] for i in range(num_clusters)}\n",
    "        for graph_idx, cluster_id in enumerate(cluster_labels):\n",
    "            cluster_to_graphs[cluster_id].append(graph_idx)\n",
    "            graph = graphs[graph_idx]  # Get the graph corresponding to the index\n",
    "        #     Check  if the graph has exactly 5 nodes\n",
    "        #     if len(graph.nodes) != 5:\n",
    "        #         raise ValueError(f\"Graph {graph_idx} in cluster {cluster_id} does not have exactly 5 nodes. It has {len(graph.nodes)} nodes.\")\n",
    "        # print(cluster_labels)\n",
    "        # Step 2: Sample one graph per cluster\n",
    "        # Compute distances for all graphs in each cluster\n",
    "        # sampled_graphs = {}\n",
    "        # k = 3\n",
    "        # for cluster in range(kmeans.n_clusters):\n",
    "        #     cluster_graphs = [graphs[i] for i in range(len(graphs)) if cluster_labels[i] == cluster]\n",
    "        # # Compute distances to centroid\n",
    "        # cluster_features = [nx.adjacency_matrix(g).toarray().flatten() for g in cluster_graphs]\n",
    "        # cluster_features - pca.fit_transform(cluster_features)\n",
    "        # print(\"Feature shape:\", cluster_features[0].shape)\n",
    "        # print(\"Cluster center shape:\", kmeans.cluster_centers_[0].shape)\n",
    "        # print(\"Shape of data used for KMeans:\", node_features.shape)\n",
    "        # distances = [np.linalg.norm(feat - kmeans.cluster_centers_[cluster]) for feat in cluster_features]\n",
    "        \n",
    "        # # Sort graphs by distance to centroid\n",
    "        # sorted_indices = np.argsort(distances)\n",
    "        \n",
    "        # # Select k graphs closest to centroid\n",
    "        # sampled_graphs = [cluster_graphs[idx] for idx in sorted_indices[:k]]\n",
    "    \n",
    "        sampled_graphs = {}\n",
    "        for cluster_id, graph_indices in cluster_to_graphs.items():\n",
    "            if graph_indices:\n",
    "                # # Get the cluster's centroid\n",
    "                # centroid = kmeans.cluster_centers_[cluster_id]\n",
    "                # # print(centroid)\n",
    "                # # Extract the features of the graphs in this cluster\n",
    "                # cluster_features = [reduced_features[i] for i in graph_indices]\n",
    "                # # Compute distances from each graph to the centroid\n",
    "                # distances = [np.linalg.norm(features - centroid) for features in cluster_features]\n",
    "                # # Find the index of the graph closest to the centroid\n",
    "                # closest_index = graph_indices[np.argmin(distances)]\n",
    "                # # Retrieve the actual graph closest to the centroid\n",
    "                # sampled_graphs[cluster_id] = graphs[closest_index]\n",
    "                sampled_graph = random.choice(graph_indices)  # Randomly sample one graph\n",
    "                sampled_graphs[cluster_id] = graphs[sampled_graph]  # Retrieve the actual graph\n",
    "        # Step 3: (Optional) Visualize sampled graphs\n",
    "        # for cluster_id, graph in sampled_graphs.items():\n",
    "        #     plt.figure(figsize=(6, 6))\n",
    "        #     nx.draw(graph, with_labels=True, node_color='skyblue', node_size=500, font_size=10)\n",
    "        #     plt.title(f\"Graph Sampled from Cluster {cluster_id}\")\n",
    "        #     plt.show()\n",
    "\n",
    "        # # Step 1: Plot all data points with cluster labels\n",
    "        # plt.figure(figsize=(10, 8))\n",
    "        # scatter = plt.scatter(\n",
    "        #     node_features[:, 0], node_features[:, 1],  # Use the original features (for visual representation, 2D plot)\n",
    "        #     c=kmeans.labels_, cmap='viridis', s=50, alpha=0.6, label=\"Graphs\"\n",
    "        # )\n",
    "\n",
    "        #     # Step 2: Plot centroids\n",
    "        # centroids = kmeans.cluster_centers_\n",
    "        # for i, centroid in enumerate(centroids):\n",
    "        #     plt.scatter(\n",
    "        #         centroid[0], centroid[1], \n",
    "        #         c='black', s=200, marker='X', label=f\"Centroid {i}\"\n",
    "        #     )\n",
    "\n",
    "        # # Step 3: Plot sampled graphs closest to centroids with matching colors\n",
    "        # for cluster_id, sampled_graph in sampled_graphs.items():\n",
    "        #     sampled_index = graphs.index(sampled_graph)  # Get the index of the sampled graph\n",
    "        #     centroid_color = plt.cm.viridis(cluster_id / len(centroids))  # Match color to centroid\n",
    "        #     plt.scatter(\n",
    "        #         node_features[sampled_index, 0], node_features[sampled_index, 1],  # Use the original features\n",
    "        #         c=[centroid_color], s=100, edgecolor='black', label=f\"Sampled Graph (Cluster {cluster_id})\"\n",
    "        #     )\n",
    "\n",
    "\n",
    "        # # Step 4: Add titles, legend, and labels\n",
    "        # plt.title(\"Graphs, Centroids, and Sampled Graphs (Original Features)\")\n",
    "        # plt.xlabel(\"Feature 1\")\n",
    "        # plt.ylabel(\"Feature 2\")  # Adjust based on what your original feature dimensions correspond to\n",
    "        # plt.legend(loc='best')\n",
    "        # plt.show()\n",
    "\n",
    "\n",
    "        # Step 4: Represent sampled graphs as adjacency matrices\n",
    "        sampled_adj_matrices = {}\n",
    "        for cluster_id, graph in sampled_graphs.items():\n",
    "            if graph:\n",
    "            # Convert the graph to an adjacency matrix\n",
    "                adj_matrix = nx.adjacency_matrix(graph).toarray()  # Convert sparse matrix to dense\n",
    "                sampled_adj_matrices[cluster_id] = adj_matrix\n",
    "\n",
    "                # Assuming `reduced_features` is your PCA-reduced data and `cluster_labels` are from KMeans\n",
    "                # `sampled_graphs` contains the sampled graphs closest to centroids\n",
    "                # `graphs` is the original dataset of graphs\n",
    "        return sampled_adj_matrices\n",
    "\n",
    "    def run_vgae(self, num_samples):\n",
    "        \n",
    "        adjacency_lists = self.getSamples(num_samples)\n",
    "        # Train on CPU (hide GPU) due to memory constraints\n",
    "        # os.environ['CUDA_VISIBLE_DEVICES'] = \"\"\n",
    "        upper_tri_values_list = [None] * self.hidden_graphs\n",
    "        adj_num = 0\n",
    "\n",
    "        features_list = []\n",
    "        self.vgae_graphs = []\n",
    "        for adj in adjacency_lists:\n",
    "            adj = adjacency_lists[0]\n",
    "            # Check if adj is a PyTorch tensor and convert it to scipy sparse matrix\n",
    "            if isinstance(adj, torch.Tensor):\n",
    "                adj = adj.cpu().numpy()  # Convert tensor to numpy array\n",
    "                adj = sp.coo_matrix(adj)  # Convert numpy array to sparse matrix\n",
    "\n",
    "            # Ensure adjacency matrix is sparse (in case it's not already)\n",
    "            if not sp.isspmatrix(adj):\n",
    "                adj = sp.coo_matrix(adj)\n",
    "\n",
    "            # Generate feature matrix (identity matrix in sparse format)\n",
    "            num_nodes = adj.shape[0]\n",
    "            features = sp.identity(num_nodes).tolil()\n",
    "\n",
    "            # Store original adjacency matrix (without diagonal entries)\n",
    "            adj_orig = adj\n",
    "            adj_orig = adj_orig - sp.dia_matrix((adj_orig.diagonal()[np.newaxis, :], [0]), shape=adj_orig.shape)\n",
    "            adj_orig.eliminate_zeros()\n",
    "\n",
    "\n",
    "            # Mask the edges (train/validation/test split)\n",
    "            train_edges, val_edges, test_edges = self.mask_test_edges(adj)\n",
    "\n",
    "            # Using the training adjacency matrix\n",
    "            adj_train = adj  # Assuming adj_train is returned or defined within mask_test_edges\n",
    "            adj_norm = preprocess_graph(adj)\n",
    "            num_nodes = adj.shape[0]\n",
    "\n",
    "            features = self.sparse_to_tuple(features.tocoo())\n",
    "            num_features = features[2][1]\n",
    "            features_nonzero = features[1].shape[0]\n",
    "\n",
    "            # Create Model\n",
    "            pos_weight = float(adj.shape[0] * adj.shape[0] - adj.sum()) / adj.sum()\n",
    "            norm = adj.shape[0] * adj.shape[0] / float((adj.shape[0] * adj.shape[0] - adj.sum()) * 2)\n",
    "\n",
    "\n",
    "            adj_label = adj_train + sp.eye(adj_train.shape[0])\n",
    "            adj_label = self.sparse_to_tuple(adj_label)\n",
    "\n",
    "\n",
    "\n",
    "            adj_norm = torch.sparse.FloatTensor(torch.LongTensor(adj_norm[0].T), \n",
    "                                        torch.FloatTensor(adj_norm[1]), \n",
    "                                        torch.Size(adj_norm[2]))\n",
    "            adj_label = torch.sparse.FloatTensor(torch.LongTensor(adj_label[0].T), \n",
    "                                        torch.FloatTensor(adj_label[1]), \n",
    "                                        torch.Size(adj_label[2]))\n",
    "            features = torch.sparse.FloatTensor(torch.LongTensor(features[0].T), \n",
    "                                        torch.FloatTensor(features[1]), \n",
    "                                        torch.Size(features[2]))\n",
    "\n",
    "            weight_mask = adj_label.to_dense().view(-1) == 1\n",
    "            weight_tensor = torch.ones(weight_mask.size(0)) \n",
    "            weight_tensor[weight_mask] = pos_weight\n",
    "\n",
    "            # init model and optimizer\n",
    "            model = getattr(vgae_model,vgae_args.model)(adj_norm, vgae_args.input_dim, vgae_args.hidden1_dim, vgae_args.hidden2_dim) \n",
    "            optimizer = Adam(model.parameters(), lr=vgae_args.learning_rate)\n",
    "            # Initialize VGAE model with the adjacency matrix\n",
    "            adj_dense = torch.tensor(adj.toarray(), dtype=torch.float32)  # Ensure it's a tensor\n",
    "            model = vgae_model.VGAE(adj_dense, vgae_args.input_dim, vgae_args.hidden1_dim, vgae_args.hidden2_dim)  # Make sure to pass the adjacency matrix during initialization\n",
    "            # Training loop\n",
    "            for epoch in range(vgae_args.num_epoch):\n",
    "                t = time.time()\n",
    "\n",
    "                # Pass the features to the model\n",
    "                A_pred = model(features)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = log_lik = norm * F.binary_cross_entropy(A_pred.view(-1), adj_label.to_dense().view(-1), weight=weight_tensor)\n",
    "                \n",
    "                if vgae_args.model == 'VGAE':\n",
    "                    # KL divergence regularization\n",
    "                    kl_divergence = 0.5 / A_pred.size(0) * (1 + 2 * model.logstd - model.mean**2 - torch.exp(model.logstd)**2).sum(1).mean()\n",
    "                    loss -= kl_divergence\n",
    "\n",
    "                # Backpropagate and optimize\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Compute train accuracy\n",
    "                train_acc = self.get_acc(A_pred, adj_label)\n",
    "\n",
    "            # Get latent embeddings (mean and log standard deviation)\n",
    "            z_mean = model.mean\n",
    "            z_log_std = model.logstd\n",
    "\n",
    "            # Sample latent variables from the Gaussian distribution\n",
    "            z_sampled = z_mean + torch.randn_like(z_log_std) * torch.exp(z_log_std)\n",
    "            features = z_sampled\n",
    "            features_list.append(features)\n",
    "\n",
    "            # Decode the sampled latent variables to generate a new adjacency matrix\n",
    "            new_adj_rec = torch.sigmoid(torch.matmul(z_sampled, z_sampled.t()))\n",
    "            # Generate binary adjacency matrix (threshold the probabilities)\n",
    "            threshold = 0.5  # Adjust the threshold as needed\n",
    "            new_adj_binary = (new_adj_rec > threshold).float()\n",
    "            G = nx.Graph()\n",
    "\n",
    "            # Add edges based on the binary adjacency matrix\n",
    "            num_nodes = new_adj_binary.shape[0]\n",
    "            for i in range(num_nodes):\n",
    "                for j in range(num_nodes):\n",
    "                    if new_adj_binary[i, j] == 1:  # If there's an edge\n",
    "                        G.add_edge(i, j)\n",
    "            self.vgae_graphs.append(G)\n",
    "            # Visualize the graph\n",
    "\n",
    "            # convert to a tensor\n",
    "            new_adj_sparse = sp.coo_matrix(new_adj_rec.detach().cpu().numpy())\n",
    "            # Ensure adj is in sparse format after edge masking\n",
    "\n",
    "            # Step 1: Convert sparse matrix to dense format\n",
    "            new_adj_dense = new_adj_sparse.toarray()  # Convert sparse COO matrix to dense array\n",
    "\n",
    "            # Step 2: Convert to PyTorch tensor\n",
    "            new_adj_tensor = torch.tensor(new_adj_dense, dtype=torch.float32, device=self.device)\n",
    "\n",
    "            # Step 3: Extract upper triangular values (excluding diagonal)\n",
    "            idx = torch.triu_indices(new_adj_tensor.size(0), new_adj_tensor.size(1), offset=1)\n",
    "            upper_tri_values = new_adj_tensor[idx[0], idx[1]]  # Extract upper triangular values\n",
    "\n",
    "            iter = 0\n",
    "\n",
    "            for cluster_label in self.cluster_labels:\n",
    "                if(cluster_label == adj_num):\n",
    "                    upper_tri_values_list[iter] = upper_tri_values\n",
    "                iter += 1\n",
    "            adj_num += 1        \n",
    "\n",
    "            # Step 4: Assign to self.adj_hidden\n",
    "            # if upper_tri_values.shape[0] != self.adj_hidden.shape[1]:\n",
    "            #     raise ValueError(f\"Size mismatch: upper_tri_values has {upper_tri_values.shape} shape, but self.adj_hidden expects {self.adj_hidden.shape}.\")\n",
    "        for i, value in enumerate(upper_tri_values_list):\n",
    "            if value is None:\n",
    "                upper_tri_values_list[i] = torch.zeros(self.adj_hidden.shape[1])\n",
    "        all_upper_tri_values = torch.stack(upper_tri_values_list, dim=0)  # Concatenate along the first dimension\n",
    "        all_features = torch.stack(features_list, dim=0)  # Concatenate along the first dimension\n",
    "        return all_upper_tri_values, all_features\n",
    "    def get_vgae_graphs(self):\n",
    "        return self.vgae_graphs\n",
    "    def get_cluster_lables(self):\n",
    "        return self.cluster_labels\n",
    "    def get_meta_graph(self):\n",
    "        return self.metagraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, test_index in kf.split(y):\n",
    "    it += 1\n",
    "    \n",
    "    idx = np.random.permutation(train_index)\n",
    "    train_index = idx[:int(idx.size*0.9)].tolist()\n",
    "    val_index = idx[int(idx.size*0.9):].tolist()\n",
    "\n",
    "    n_train = len(train_index)\n",
    "    n_val = len(val_index)\n",
    "    n_test = len(test_index)\n",
    "\n",
    "    adj_train = [adj_lst[i] for i in train_index]\n",
    "    features_train = [features_lst[i] for i in train_index]\n",
    "    y_train = [y[i] for i in train_index]\n",
    "\n",
    "    adj_val = [adj_lst[i] for i in val_index]\n",
    "    features_val = [features_lst[i] for i in val_index]\n",
    "    y_val = [y[i] for i in val_index]\n",
    "\n",
    "    adj_test = [adj_lst[i] for i in test_index]\n",
    "    features_test = [features_lst[i] for i in test_index]\n",
    "    y_test = [y[i] for i in test_index]\n",
    "\n",
    "    adj_train, features_train, graph_indicator_train, y_train = generate_batches(adj_train, features_train, y_train, args.batch_size, device)\n",
    "    adj_val, features_val, graph_indicator_val, y_val = generate_batches(adj_val, features_val, y_val, args.batch_size, device)\n",
    "    adj_test, features_test, graph_indicator_test, y_test = generate_batches(adj_test, features_test, y_test, args.batch_size, device)\n",
    "\n",
    "    n_train_batches = ceil(n_train/args.batch_size)\n",
    "    n_val_batches = ceil(n_val/args.batch_size)\n",
    "    n_test_batches = ceil(n_test/args.batch_size)\n",
    "    \n",
    "    model = RW_NN(features_dim, args.max_step, args.hidden_graphs, args.size_hidden_graphs, args.hidden_dim, args.penultimate_dim, args.normalize, n_classes, args.dropout, device).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)\n",
    "\n",
    "    def train(epoch, adj, features, graph_indicator, y):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(adj, features, graph_indicator)\n",
    "        loss_train = F.cross_entropy(output, y)\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "        return output, loss_train\n",
    "\n",
    "    def test(adj, features, graph_indicator, y):\n",
    "        output = model(adj, features, graph_indicator)\n",
    "        loss_test = F.cross_entropy(output, y)\n",
    "        return output, loss_test\n",
    "\n",
    "    best_acc = 0\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        start = time.time()\n",
    "        model.train()\n",
    "        train_loss = AverageMeter()\n",
    "        train_acc = AverageMeter()\n",
    "\n",
    "        # Train for one epoch\n",
    "        for i in range(n_train_batches):\n",
    "            output, loss = train(epoch, adj_train[i], features_train[i], graph_indicator_train[i], y_train[i])\n",
    "            train_loss.update(loss.item(), output.size(0))\n",
    "            train_acc.update(accuracy(output.data, y_train[i].data), output.size(0))\n",
    "\n",
    "        # Evaluate on validation set every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0 or epoch == args.epochs - 1:  # Always evaluate at the last epoch\n",
    "            model.eval()\n",
    "            val_loss = AverageMeter()\n",
    "            val_acc = AverageMeter()\n",
    "\n",
    "            for i in range(n_val_batches):\n",
    "                output, loss = test(adj_val[i], features_val[i], graph_indicator_val[i], y_val[i])\n",
    "                val_loss.update(loss.item(), output.size(0))\n",
    "                val_acc.update(accuracy(output.data, y_val[i].data), output.size(0))\n",
    "\n",
    "            # Print validation results\n",
    "            print(\"Cross-val iter:\", '%02d' % it, \"epoch:\", '%03d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(train_loss.avg),\n",
    "                \"train_acc=\", \"{:.5f}\".format(train_acc.avg), \"val_loss=\", \"{:.5f}\".format(val_loss.avg),\n",
    "                \"val_acc=\", \"{:.5f}\".format(val_acc.avg), \"time=\", \"{:.5f}\".format(time.time() - start))\n",
    "\n",
    "            # Remember best accuracy and save checkpoint\n",
    "            is_best = val_acc.avg >= best_acc\n",
    "            best_acc = max(val_acc.avg, best_acc)\n",
    "            if is_best:\n",
    "                early_stopping_counter = 0\n",
    "                torch.save({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'state_dict': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                }, 'model_best.pth.tar')\n",
    "\n",
    "        # Adjust learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "    print(\"Optimization finished!\")\n",
    "\n",
    "    # Testing\n",
    "    test_loss = AverageMeter()\n",
    "    test_acc = AverageMeter()\n",
    "    print(\"Loading checkpoint!\")\n",
    "    checkpoint = torch.load('model_best.pth.tar')\n",
    "    epoch = checkpoint['epoch']\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "\n",
    "    for i in range(n_test_batches):\n",
    "        output, loss = test(adj_test[i], features_test[i], graph_indicator_test[i], y_test[i])\n",
    "        test_loss.update(loss.item(), output.size(0))\n",
    "        test_acc.update(accuracy(output.data, y_test[i].data), output.size(0))\n",
    "    accs.append(test_acc.avg.cpu().numpy())\n",
    "\n",
    "    # Print results\n",
    "    print(\"test_loss=\", \"{:.5f}\".format(test_loss.avg), \"test_acc=\", \"{:.5f}\".format(test_acc.avg))\n",
    "    print()\n",
    "\n",
    "    print(\"avg_test_acc=\", \"{:.5f}\".format(np.mean(accs)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Graph Cluster Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Assuming adjacency_lists is a list of dictionaries where each dictionary\n",
    "# represents the adjacency list of a hidden graph.\n",
    "adjacency_lists = model.get_hidden_graphs_adjacency_list()\n",
    "cluster_labels = model.get_cluster_lables()\n",
    "\n",
    "# Define a mapping from clusters to pale colors\n",
    "# Ensure there are enough colors by generating more if needed\n",
    "unique_clusters = sorted(set(cluster_labels))\n",
    "colors = plt.cm.viridis(np.linspace(0.5, 1, len(unique_clusters)))  # Generates enough distinct colors\n",
    "\n",
    "cluster_to_color = {cluster: colors[i] for i, cluster in enumerate(unique_clusters)}\n",
    "print(cluster_to_color)\n",
    "# Pair the adjacency lists with their corresponding cluster labels and sort by cluster label\n",
    "sorted_graphs = sorted(\n",
    "    enumerate(adjacency_lists), \n",
    "    key=lambda x: cluster_labels[x[0]]\n",
    ")\n",
    "sorted_adjacency_lists = [adjacency_lists[i] for i, _ in sorted_graphs]\n",
    "sorted_cluster_labels = [cluster_labels[i] for i, _ in sorted_graphs]\n",
    "\n",
    "# Number of graphs\n",
    "num_graphs = len(sorted_adjacency_lists)\n",
    "\n",
    "# Determine grid size for displaying graphs (e.g., 4x4 for 16 graphs)\n",
    "cols = 4  # Number of columns in the grid\n",
    "rows = (num_graphs + cols - 1) // cols  # Calculate the required number of rows\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(20, 15))\n",
    "axes = axes.flatten()  # Flatten to make indexing easier\n",
    "\n",
    "# Set figure background to white\n",
    "fig.patch.set_facecolor('white')\n",
    "\n",
    "# Iterate through each hidden graph\n",
    "for i, (adj_list, cluster_label) in enumerate(zip(sorted_adjacency_lists, sorted_cluster_labels)):\n",
    "    G = nx.Graph()  # Create a new graph for each hidden graph\n",
    "    \n",
    "    # Add edges based on the adjacency list\n",
    "    for node, neighbors in adj_list.items():\n",
    "        for neighbor in neighbors:\n",
    "            G.add_edge(node, neighbor)\n",
    "    \n",
    "    # Get the current axis\n",
    "    ax = axes[i]\n",
    "    # Set subplot background to white\n",
    "    ax.set_facecolor(cluster_to_color[cluster_label])\n",
    "    \n",
    "    # Compute positions for nodes\n",
    "    pos = nx.spring_layout(G)\n",
    "    \n",
    "    # Draw edges with increased width and distinct color    \n",
    "    # Draw nodes and labels\n",
    "    nx.draw_networkx_nodes(G, pos, ax=ax, node_color='skyblue', node_size=500)\n",
    "    nx.draw_networkx_edges(G, pos, ax=ax, edge_color='black', width=1)\n",
    "\n",
    "    nx.draw_networkx_labels(G, pos, ax=ax, font_size=8)\n",
    "    \n",
    "    # Adjust axis limits to ensure edges are visible\n",
    "    x_vals, y_vals = zip(*pos.values())\n",
    "    ax.set_xlim(min(x_vals) - 0.1, max(x_vals) + 0.1)\n",
    "    ax.set_ylim(min(y_vals) - 0.1, max(y_vals) + 0.1)\n",
    "    \n",
    "    # Remove axis ticks\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "    # Add a border around the subplot\n",
    "    ax.add_patch(Rectangle(\n",
    "        (0, 0), 1, 1, transform=ax.transAxes, \n",
    "        linewidth=2, edgecolor='black', facecolor='none'\n",
    "    ))\n",
    "    \n",
    "    # Set the title\n",
    "    ax.set_title(f\"Hidden Graph {i}\\nCluster {cluster_label}\", fontsize=10)\n",
    "\n",
    "# Hide any unused subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = model.get_meta_graph()\n",
    "pos = nx.spring_layout(G)\n",
    "\n",
    "nx.draw_networkx_nodes(G, pos)\n",
    "\n",
    "labels = nx.get_edge_attributes(G, 'weight')\n",
    "nx.draw_networkx_edges(G, pos)\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels=labels)\n",
    "plt.show()\n",
    "\n",
    "cluster_labels = model.get_cluster_lables()\n",
    "# visualize metagraph with cluster labels\n",
    "pos = nx.spring_layout(G)  # Spring layout for visualization\n",
    "nx.draw_networkx_nodes(G, pos, node_color=cluster_labels, cmap='viridis', node_size=300)\n",
    "nx.draw_networkx_edges(G, pos, alpha=0.5)\n",
    "\n",
    "# Draw edge labels (weights)\n",
    "weights = nx.get_edge_attributes(G, \"weight\")\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels=weights, font_size=8)\n",
    "\n",
    "\n",
    "# Add labels (optional)\n",
    "nx.draw_networkx_labels(G, pos, font_size=10)\n",
    "plt.title(\"Clustering of Meta-Graph Nodes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGAE Hidden Graph Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from matplotlib.patches import Rectangle\n",
    "import matplotlib.colors as mcolors\n",
    "import numpy as np  # For np.linspace if not already imported\n",
    "\n",
    "# Assuming vgae_graphs is a list of networkx graph objects\n",
    "vgae_graphs = model.get_vgae_graphs()\n",
    "# Generate a distinct set of colors for each graph\n",
    "unique_clusters = sorted(set(range(len(vgae_graphs))))  # Using the index as the cluster label\n",
    "colors = plt.cm.viridis(np.linspace(0.5, 1, len(unique_clusters)))  # Generates enough distinct colors\n",
    "cluster_to_color = {cluster: colors[i] for i, cluster in enumerate(unique_clusters)}\n",
    "\n",
    "# Determine grid size for displaying graphs (e.g., 4x4 for 16 graphs)\n",
    "cols = 4  # Number of columns in the grid\n",
    "rows = (len(vgae_graphs) + cols - 1) // cols  # Calculate the required number of rows\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(20, 5))\n",
    "axes = axes.flatten()  # Flatten to make indexing easier\n",
    "\n",
    "# Set figure background to white\n",
    "fig.patch.set_facecolor('white')\n",
    "\n",
    "# Iterate through each VGAE graph\n",
    "for i, graph in enumerate(vgae_graphs):\n",
    "    ax = axes[i]  # Get the current axis\n",
    "    G = graph\n",
    "    \n",
    "    # Add edges based on the adjacency list\n",
    "    cluster_label = i  # Cluster is the index of the graph\n",
    "    \n",
    "    # Set background color for the axis\n",
    "    ax.set_facecolor(cluster_to_color[cluster_label])  # Set background color based on cluster\n",
    "    pos = nx.spring_layout(G)\n",
    "    \n",
    "    # Draw edges with increased width and distinct color    \n",
    "    # Draw nodes and labels\n",
    "    nx.draw_networkx_nodes(G, pos, ax=ax, node_color='skyblue', node_size=500)\n",
    "    nx.draw_networkx_edges(G, pos, ax=ax, edge_color='black', width=1)\n",
    "\n",
    "    nx.draw_networkx_labels(G, pos, ax=ax, font_size=8)\n",
    "    \n",
    "    x_vals, y_vals = zip(*pos.values())\n",
    "    ax.set_xlim(min(x_vals) - 0.1, max(x_vals) + 0.1)\n",
    "    ax.set_ylim(min(y_vals) - 0.1, max(y_vals) + 0.1)\n",
    "    # Draw the graph with default node and edge colors\n",
    "    # nx.draw(\n",
    "    #     graph, with_labels=True, node_color='skyblue', edge_color='gray',\n",
    "    #     node_size=2000, font_size=10, ax=ax\n",
    "    # )\n",
    "    \n",
    "    # Set the title for each subplot\n",
    "    ax.set_title(f\"Graph {i} from Decoded Adjacency Matrix\\nCluster {cluster_label}\")\n",
    "\n",
    "    # Remove axis ticks for a cleaner view\n",
    "    ax.set_xticks([]) \n",
    "    ax.set_yticks([])\n",
    "\n",
    "    # Add a border around the subplot (optional)\n",
    "    ax.add_patch(Rectangle(\n",
    "        (0, 0), 1, 1, transform=ax.transAxes, \n",
    "        linewidth=2, edgecolor='black', facecolor='none'\n",
    "    ))\n",
    "\n",
    "# Hide any unused subplots (if there are fewer graphs than grid spaces)\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "36venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
