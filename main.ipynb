{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "# from model import RW_NN\n",
    "from utils import load_data, generate_batches, accuracy, AverageMeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.1+cu110\n",
      "CUDA Available: True\n",
      "Number of GPUs available: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"Number of GPUs available:\", torch.cuda.device_count())\n",
    "# print(\"CUDA Device Name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "from types import SimpleNamespace\n",
    "args = SimpleNamespace(\n",
    "    dataset='synthetic',\n",
    "    use_node_labels=False,\n",
    "    lr=1e-2,\n",
    "    dropout=0.2,\n",
    "    batch_size=64,\n",
    "    epochs=100,\n",
    "    hidden_graphs=16,\n",
    "    size_hidden_graphs=5,\n",
    "    hidden_dim=4,\n",
    "    penultimate_dim=32,\n",
    "    max_step=1,\n",
    "    normalize=False,\n",
    "    num_samples = 8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_lst, features_lst, class_labels = load_data(args.dataset, args.use_node_labels)\n",
    "\n",
    "N = len(adj_lst)\n",
    "features_dim = features_lst[0].shape[1]\n",
    "\n",
    "enc = LabelEncoder()\n",
    "class_labels = enc.fit_transform(class_labels)\n",
    "n_classes = np.unique(class_labels).size\n",
    "y = [np.array(class_labels[i]) for i in range(class_labels.size)]\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=13)\n",
    "it = 0\n",
    "accs = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE MODEL\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import networkx as nx\n",
    "from input_data import load_data\n",
    "from preprocessing import preprocess_graph\n",
    "import vgae_model\n",
    "from types import SimpleNamespace\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import random\n",
    "\n",
    "vgae_args = SimpleNamespace(\n",
    "    ### CONFIGS ###\n",
    "    model = 'VGAE',\n",
    "    input_dim = args.size_hidden_graphs, \n",
    "    hidden1_dim = 4,\n",
    "    hidden2_dim = 3,\n",
    "    use_feature = True,\n",
    "    num_epoch = 100,\n",
    "    learning_rate = 0.01\n",
    ")\n",
    "\n",
    "class RW_NN(nn.Module):\n",
    "    # new hyperparameters\n",
    "    # max_num_children\n",
    "    def __init__(self, input_dim, max_step, hidden_graphs, size_hidden_graphs, hidden_dim, penultimate_dim, normalize, n_classes, dropout, device):\n",
    "        super(RW_NN, self).__init__()\n",
    "        self.max_step = max_step\n",
    "        self.hidden_graphs = hidden_graphs\n",
    "        self.size_hidden_graphs = size_hidden_graphs\n",
    "        self.normalize = normalize\n",
    "        self.device = device\n",
    "\n",
    "        self.adj_hidden = Parameter(torch.FloatTensor(hidden_graphs, (size_hidden_graphs*(size_hidden_graphs-1))//2))\n",
    "        self.adj_hidden_tree = None\n",
    "        self.adj_hidden_tree_norm = None\n",
    "        self.features_hidden = Parameter(torch.FloatTensor(hidden_graphs, size_hidden_graphs, hidden_dim))\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.features_hidden_tree = None\n",
    "        self.input_dim = input_dim\n",
    "        self.penultimate_dim = penultimate_dim\n",
    "\n",
    "        self.fc = torch.nn.Linear(input_dim, hidden_dim)\n",
    "        self.bn = nn.BatchNorm1d(hidden_graphs*max_step)\n",
    "        self.fc1 = torch.nn.Linear(hidden_graphs*max_step, penultimate_dim)\n",
    "        self.fc2 = torch.nn.Linear(penultimate_dim, n_classes)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.adj_hidden.data.uniform_(-1, 1)\n",
    "        self.features_hidden.data.uniform_(0, 1)\n",
    "\n",
    "        \n",
    "    def forward(self, adj, features, graph_indicator, y=None):\n",
    "        if self.training:            \n",
    "            self.fc = torch.nn.Linear(self.input_dim, self.hidden_dim)\n",
    "            self.bn = nn.BatchNorm1d(self.hidden_graphs*self.max_step)\n",
    "            self.fc1 = torch.nn.Linear(self.hidden_graphs*self.max_step, self.penultimate_dim)\n",
    "            self.fc2 = torch.nn.Linear(self.penultimate_dim, n_classes)\n",
    "            unique, counts = torch.unique(graph_indicator, return_counts=True)\n",
    "            n_graphs = unique.size(0)\n",
    "            n_nodes = features.size(0)\n",
    "\n",
    "            if self.normalize:\n",
    "                norm = counts.unsqueeze(1).repeat(1, self.hidden_graphs)\n",
    "            \n",
    "            adj_hidden_norm = torch.zeros(self.hidden_graphs, self.size_hidden_graphs, self.size_hidden_graphs).to(self.device)\n",
    "            idx = torch.triu_indices(self.size_hidden_graphs, self.size_hidden_graphs, 1)\n",
    "            adj_hidden_norm[:,idx[0],idx[1]] = self.relu(self.adj_hidden)\n",
    "            adj_hidden_norm = adj_hidden_norm + torch.transpose(adj_hidden_norm, 1, 2)\n",
    "            x = self.sigmoid(self.fc(features))\n",
    "            z = self.features_hidden\n",
    "            zx = torch.einsum(\"abc,dc->abd\", (z, x))\n",
    "            \n",
    "            out = list()\n",
    "            for i in range(self.max_step):\n",
    "                if i == 0:\n",
    "                    eye = torch.eye(self.size_hidden_graphs, device=self.device)\n",
    "                    eye = eye.repeat(self.hidden_graphs, 1, 1)              \n",
    "                    o = torch.einsum(\"abc,acd->abd\", (eye, z))\n",
    "                    t = torch.einsum(\"abc,dc->abd\", (o, x))\n",
    "                else:\n",
    "                    x = torch.spmm(adj, x)\n",
    "                    z = torch.einsum(\"abc,acd->abd\", (adj_hidden_norm, z))\n",
    "                    t = torch.einsum(\"abc,dc->abd\", (z, x))\n",
    "                t = self.dropout(t)\n",
    "                t = torch.mul(zx, t)\n",
    "                t = torch.zeros(t.size(0), t.size(1), n_graphs, device=self.device).index_add_(2, graph_indicator, t)\n",
    "                t = torch.sum(t, dim=1)\n",
    "                t = torch.transpose(t, 0, 1)\n",
    "                if self.normalize:\n",
    "                    t /= norm\n",
    "                out.append(t)\n",
    "                \n",
    "            out = torch.cat(out, dim=1)\n",
    "            out = self.bn(out)\n",
    "            out = self.relu(self.fc1(out))\n",
    "            out = self.dropout(out)\n",
    "            out = self.fc2(out)\n",
    "            return F.log_softmax(out, dim=1)\n",
    "        else:\n",
    "            num_samples = args.num_samples\n",
    "            adj_hidden_vgae, vgae_features = self.run_vgae(num_samples)\n",
    "            self.fc = torch.nn.Linear(self.input_dim, self.hidden_dim)\n",
    "            self.bn = nn.BatchNorm1d(num_samples*self.max_step)\n",
    "            self.fc1 = torch.nn.Linear(num_samples*self.max_step, self.penultimate_dim)\n",
    "            self.fc2 = torch.nn.Linear(self.penultimate_dim, n_classes)\n",
    "            unique, counts = torch.unique(graph_indicator, return_counts=True)\n",
    "            n_graphs = unique.size(0)\n",
    "            n_nodes = features.size(0)\n",
    "\n",
    "            if self.normalize:\n",
    "                norm = counts.unsqueeze(1).repeat(1, self.hidden_graphs)\n",
    "\n",
    "\n",
    "            adj_hidden_norm = torch.zeros(num_samples, self.size_hidden_graphs, self.size_hidden_graphs).to(self.device)\n",
    "            idx = torch.triu_indices(self.size_hidden_graphs, self.size_hidden_graphs, 1)\n",
    "            adj_hidden_norm[:,idx[0],idx[1]] = self.relu(adj_hidden_vgae)\n",
    "            adj_hidden_norm = adj_hidden_norm + torch.transpose(adj_hidden_norm, 1, 2)\n",
    "            x = self.sigmoid(self.fc(features))\n",
    "            z = vgae_features\n",
    "            zx = torch.einsum(\"abc,dc->abd\", (z, x))\n",
    "            \n",
    "            out = list()\n",
    "            for i in range(self.max_step):\n",
    "                if i == 0:\n",
    "                    eye = torch.eye(self.size_hidden_graphs, device=self.device)\n",
    "                    eye = eye.repeat(num_samples, 1, 1)              \n",
    "                    o = torch.einsum(\"abc,acd->abd\", (eye, z))\n",
    "                    t = torch.einsum(\"abc,dc->abd\", (o, x))\n",
    "                else:\n",
    "                    x = torch.spmm(adj, x)\n",
    "                    z = torch.einsum(\"abc,acd->abd\", (adj_hidden_norm, z))\n",
    "                    t = torch.einsum(\"abc,dc->abd\", (z, x))\n",
    "                t = self.dropout(t)\n",
    "                t = torch.mul(zx, t)\n",
    "                t = torch.zeros(t.size(0), t.size(1), n_graphs, device=self.device).index_add_(2, graph_indicator, t)\n",
    "                t = torch.sum(t, dim=1)\n",
    "                t = torch.transpose(t, 0, 1)\n",
    "                if self.normalize:\n",
    "                    t /= norm\n",
    "                out.append(t)\n",
    "                \n",
    "            out = torch.cat(out, dim=1)\n",
    "            out = self.bn(out)\n",
    "            out = self.relu(self.fc1(out))\n",
    "            out = self.dropout(out)\n",
    "            out = self.fc2(out)\n",
    "            return F.log_softmax(out, dim=1)\n",
    "        \n",
    "\n",
    "    \n",
    "    def get_hidden_graphs_adjacency_list(self):\n",
    "        \"\"\"Converts the upper-triangular adjacency data to adjacency lists.\"\"\"\n",
    "        adj_hidden_norm = torch.zeros(self.hidden_graphs, self.size_hidden_graphs, self.size_hidden_graphs).to(self.device)\n",
    "        idx = torch.triu_indices(self.size_hidden_graphs, self.size_hidden_graphs, 1)\n",
    "        adj_hidden_norm[:, idx[0], idx[1]] = self.relu(self.adj_hidden)\n",
    "        adj_hidden_norm = adj_hidden_norm + torch.transpose(adj_hidden_norm, 1, 2)\n",
    "        \n",
    "        # Convert each hidden graph's adjacency matrix to an adjacency list\n",
    "        adjacency_lists = []\n",
    "        for i in range(self.hidden_graphs):\n",
    "            adj_list = {}\n",
    "            adj_matrix = adj_hidden_norm[i].detach().cpu().numpy()  # Move to CPU for easy processing\n",
    "            for row in range(adj_matrix.shape[0]):\n",
    "                adj_list[row] = list(np.where(adj_matrix[row] > 0)[0])  # Find connected nodes\n",
    "            adjacency_lists.append(adj_list)\n",
    "        \n",
    "        return adjacency_lists\n",
    "    \n",
    "\n",
    "\n",
    "# VGAE METHODS\n",
    "    def sparse_to_tuple(self, sparse_mx):\n",
    "        if not sp.isspmatrix_coo(sparse_mx):\n",
    "            sparse_mx = sparse_mx.tocoo()\n",
    "        coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n",
    "        values = sparse_mx.data\n",
    "        shape = sparse_mx.shape\n",
    "        return coords, values, shape\n",
    "    def mask_test_edges(self, adj):\n",
    "        adj = adj - sp.dia_matrix((adj.diagonal()[np.newaxis, :], [0]), shape=adj.shape)\n",
    "        adj.eliminate_zeros()\n",
    "\n",
    "        adj_triu = sp.triu(adj)\n",
    "        adj_tuple = self.sparse_to_tuple(adj_triu)\n",
    "        edges = adj_tuple[0]\n",
    "\n",
    "        num_edges = edges.shape[0]\n",
    "        if num_edges < 10:\n",
    "            # print(\"Not enough edges for a split. Using all edges for training.\")\n",
    "            return edges, [], []  # Return all edges for training, no validation/test\n",
    "\n",
    "        num_test = int(np.floor(num_edges / 10.))\n",
    "        num_val = int(np.floor(num_edges / 20.))\n",
    "\n",
    "        all_edge_idx = list(range(num_edges))\n",
    "        np.random.shuffle(all_edge_idx)\n",
    "        val_edge_idx = all_edge_idx[:num_val]\n",
    "        test_edge_idx = all_edge_idx[num_val:(num_val + num_test)]\n",
    "\n",
    "        test_edges = edges[test_edge_idx] if test_edge_idx else []\n",
    "        val_edges = edges[val_edge_idx] if val_edge_idx else []\n",
    "        if test_edge_idx or val_edge_idx:\n",
    "            train_edges = np.delete(edges, np.hstack([test_edge_idx, val_edge_idx]), axis=0)\n",
    "        else:\n",
    "            train_edges = edges\n",
    "\n",
    "        return train_edges, val_edges, test_edges\n",
    "    def get_acc(self, adj_rec, adj_label):\n",
    "        labels_all = adj_label.to_dense().view(-1).long()\n",
    "        preds_all = (adj_rec > 0.5).view(-1).long()\n",
    "        accuracy = (preds_all == labels_all).sum().float() / labels_all.size(0)\n",
    "        return accuracy\n",
    "    \n",
    "    def genMetaGraphs(self, graph_dist_adj_matrix):\n",
    "        G = nx.from_numpy_array(graph_dist_adj_matrix)\n",
    "\n",
    "        pos = nx.spring_layout(G)\n",
    "\n",
    "        nx.draw_networkx_nodes(G, pos)\n",
    "\n",
    "        labels = nx.get_edge_attributes(G, 'weight')\n",
    "        nx.draw_networkx_edges(G, pos)\n",
    "        nx.draw_networkx_edge_labels(G, pos, edge_labels=labels)\n",
    "\n",
    "        plt.show()\n",
    "    # NUM_SAMPLES = NUM_CLUSTERS\n",
    "    def getSamples(self, num_samples):\n",
    "        adjacency_lists = self.get_hidden_graphs_adjacency_list()\n",
    "        for graph_id, adj_list in enumerate(adjacency_lists):\n",
    "            if len(adj_list) != 5:\n",
    "                print(\"FIRST ONE NOT GOOD\")\n",
    "                raise ValueError(f\"Graph {graph_id} does not have exactly 5 nodes. It has {len(adj_list)} nodes.\")\n",
    "        graphs = []\n",
    "\n",
    "        # Draw the adjacency list as a graph using NetworkX and Matplotlib\n",
    "        for i, adj_list in enumerate(adjacency_lists):\n",
    "            G = nx.Graph()  # Create a new graph\n",
    "            # Add edges to the graph based on the adjacency list\n",
    "            for node, neighbors in adj_list.items():\n",
    "                G.add_node(node)\n",
    "                for neighbor in neighbors:\n",
    "                    G.add_edge(node, neighbor)\n",
    "\n",
    "            if len(G.nodes) != 5:\n",
    "                raise ValueError(f\"Graph {i} does not have exactly 5 nodes. It has {len(G.nodes)} nodes.\")\n",
    "\n",
    "            # for j, graph in enumerate(graphs):\n",
    "            #     if nx.is_isomorphic(graph, G):\n",
    "            #         print(\"Graph\", i, \"is isomorphic to previous graph\", j)\n",
    "            graphs.append(G)\n",
    "\n",
    "        #Edit distance\n",
    "        graph_dist_adj_matrix = np.zeros(shape=(len(graphs),len(graphs)))\n",
    "\n",
    "        for i, graph_1 in enumerate(graphs):\n",
    "            for j, graph_2 in enumerate(graphs):\n",
    "                if i == j:\n",
    "                    continue\n",
    "\n",
    "                graph_edit_distance = nx.graph_edit_distance(graph_1, graph_2)\n",
    "                # print('Edit distance between graph', i, ' and graph', j, ':', graph_edit_distance)\n",
    "                graph_dist_adj_matrix[i, j] = graph_edit_distance\n",
    "        \n",
    "        # self.genMetaGraphs(graph_dist_adj_matrix)\n",
    "        # Perform clustering\n",
    "        # use metagraph adjacency matrix as node features\n",
    "        node_features = graph_dist_adj_matrix\n",
    "\n",
    "        # apply PCA for dimensionality reduction\n",
    "        pca = PCA(n_components=2)  # Reduce to 2 components for visualization\n",
    "        reduced_features = pca.fit_transform(node_features)\n",
    "\n",
    "        # apply clustering\n",
    "        num_clusters = num_samples\n",
    "        kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "        cluster_labels = kmeans.fit_predict(reduced_features)\n",
    "\n",
    "        # visualize metagraph with cluster labels\n",
    "        # pos = nx.spring_layout(G)  # Spring layout for visualization\n",
    "        # nx.draw_networkx_nodes(G, pos, node_color=cluster_labels, cmap='viridis', node_size=300)\n",
    "        # nx.draw_networkx_edges(G, pos, alpha=0.5)\n",
    "\n",
    "        # Add labels (optional)\n",
    "        # nx.draw_networkx_labels(G, pos, font_size=10)\n",
    "        # plt.title(\"Clustering of Meta-Graph Nodes\")\n",
    "        # plt.show()\n",
    "        # Step 1: Group graph indices by cluster labels\n",
    "        cluster_to_graphs = {i: [] for i in range(num_clusters)}\n",
    "        for graph_idx, cluster_id in enumerate(cluster_labels):\n",
    "            cluster_to_graphs[cluster_id].append(graph_idx)\n",
    "            graph = graphs[graph_idx]  # Get the graph corresponding to the index\n",
    "            # Check  if the graph has exactly 5 nodes\n",
    "            if len(graph.nodes) != 5:\n",
    "                raise ValueError(f\"Graph {graph_idx} in cluster {cluster_id} does not have exactly 5 nodes. It has {len(graph.nodes)} nodes.\")\n",
    "\n",
    "        # Step 2: Sample one graph per cluster\n",
    "        sampled_graphs = {}\n",
    "        for cluster_id, graph_indices in cluster_to_graphs.items():\n",
    "            sampled_graph = random.choice(graph_indices)  # Randomly sample one graph\n",
    "            sampled_graphs[cluster_id] = graphs[sampled_graph]  # Retrieve the actual graph\n",
    "\n",
    "        # Step 3: (Optional) Visualize sampled graphs\n",
    "        # for cluster_id, graph in sampled_graphs.items():\n",
    "        #     plt.figure(figsize=(6, 6))\n",
    "        #     nx.draw(graph, with_labels=True, node_color='skyblue', node_size=500, font_size=10)\n",
    "        #     plt.title(f\"Graph Sampled from Cluster {cluster_id}\")\n",
    "        #     plt.show()\n",
    "\n",
    "        # Step 4: Represent sampled graphs as adjacency matrices\n",
    "        sampled_adj_matrices = {}\n",
    "        for cluster_id, graph in sampled_graphs.items():\n",
    "            # Convert the graph to an adjacency matrix\n",
    "            adj_matrix = nx.adjacency_matrix(graph).toarray()  # Convert sparse matrix to dense\n",
    "            sampled_adj_matrices[cluster_id] = adj_matrix\n",
    "\n",
    "        return sampled_adj_matrices\n",
    "\n",
    "    def run_vgae(self, num_samples):\n",
    "        adjacency_lists = self.getSamples(num_samples)\n",
    " \n",
    "        # Train on CPU (hide GPU) due to memory constraints\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = \"\"\n",
    "        upper_tri_values_list = []\n",
    "        features_list = []\n",
    "        for adj in adjacency_lists:\n",
    "            adj = adjacency_lists[0]\n",
    "            # Check if adj is a PyTorch tensor and convert it to scipy sparse matrix\n",
    "            if isinstance(adj, torch.Tensor):\n",
    "                adj = adj.cpu().numpy()  # Convert tensor to numpy array\n",
    "                adj = sp.coo_matrix(adj)  # Convert numpy array to sparse matrix\n",
    "\n",
    "            # Ensure adjacency matrix is sparse (in case it's not already)\n",
    "            if not sp.isspmatrix(adj):\n",
    "                adj = sp.coo_matrix(adj)\n",
    "\n",
    "            # Generate feature matrix (identity matrix in sparse format)\n",
    "            num_nodes = adj.shape[0]\n",
    "            features = sp.identity(num_nodes).tolil()\n",
    "\n",
    "            # Store original adjacency matrix (without diagonal entries)\n",
    "            adj_orig = adj\n",
    "            adj_orig = adj_orig - sp.dia_matrix((adj_orig.diagonal()[np.newaxis, :], [0]), shape=adj_orig.shape)\n",
    "            adj_orig.eliminate_zeros()\n",
    "\n",
    "\n",
    "            # Mask the edges (train/validation/test split)\n",
    "            train_edges, val_edges, test_edges = self.mask_test_edges(adj)\n",
    "\n",
    "            # Using the training adjacency matrix\n",
    "            adj_train = adj  # Assuming adj_train is returned or defined within mask_test_edges\n",
    "            adj_norm = preprocess_graph(adj)\n",
    "            num_nodes = adj.shape[0]\n",
    "\n",
    "            features = self.sparse_to_tuple(features.tocoo())\n",
    "            num_features = features[2][1]\n",
    "            features_nonzero = features[1].shape[0]\n",
    "\n",
    "            # Create Model\n",
    "            pos_weight = float(adj.shape[0] * adj.shape[0] - adj.sum()) / adj.sum()\n",
    "            norm = adj.shape[0] * adj.shape[0] / float((adj.shape[0] * adj.shape[0] - adj.sum()) * 2)\n",
    "\n",
    "\n",
    "            adj_label = adj_train + sp.eye(adj_train.shape[0])\n",
    "            adj_label = self.sparse_to_tuple(adj_label)\n",
    "\n",
    "\n",
    "\n",
    "            adj_norm = torch.sparse.FloatTensor(torch.LongTensor(adj_norm[0].T), \n",
    "                                        torch.FloatTensor(adj_norm[1]), \n",
    "                                        torch.Size(adj_norm[2]))\n",
    "            adj_label = torch.sparse.FloatTensor(torch.LongTensor(adj_label[0].T), \n",
    "                                        torch.FloatTensor(adj_label[1]), \n",
    "                                        torch.Size(adj_label[2]))\n",
    "            features = torch.sparse.FloatTensor(torch.LongTensor(features[0].T), \n",
    "                                        torch.FloatTensor(features[1]), \n",
    "                                        torch.Size(features[2]))\n",
    "\n",
    "            weight_mask = adj_label.to_dense().view(-1) == 1\n",
    "            weight_tensor = torch.ones(weight_mask.size(0)) \n",
    "            weight_tensor[weight_mask] = pos_weight\n",
    "\n",
    "            # init model and optimizer\n",
    "            model = getattr(vgae_model,vgae_args.model)(adj_norm)\n",
    "            optimizer = Adam(model.parameters(), lr=vgae_args.learning_rate)\n",
    "            # Initialize VGAE model with the adjacency matrix\n",
    "            adj_dense = torch.tensor(adj.toarray(), dtype=torch.float32)  # Ensure it's a tensor\n",
    "            model = vgae_model.VGAE(adj_dense)  # Make sure to pass the adjacency matrix during initialization\n",
    "            # Training loop\n",
    "            for epoch in range(vgae_args.num_epoch):\n",
    "                t = time.time()\n",
    "\n",
    "                # Pass the features to the model\n",
    "                A_pred = model(features)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = log_lik = norm * F.binary_cross_entropy(A_pred.view(-1), adj_label.to_dense().view(-1), weight=weight_tensor)\n",
    "                \n",
    "                if vgae_args.model == 'VGAE':\n",
    "                    # KL divergence regularization\n",
    "                    kl_divergence = 0.5 / A_pred.size(0) * (1 + 2 * model.logstd - model.mean**2 - torch.exp(model.logstd)**2).sum(1).mean()\n",
    "                    loss -= kl_divergence\n",
    "\n",
    "                # Backpropagate and optimize\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Compute train accuracy\n",
    "                train_acc = self.get_acc(A_pred, adj_label)\n",
    "\n",
    "            # Get latent embeddings (mean and log standard deviation)\n",
    "            z_mean = model.mean\n",
    "            z_log_std = model.logstd\n",
    "\n",
    "            # Sample latent variables from the Gaussian distribution\n",
    "            z_sampled = z_mean + torch.randn_like(z_log_std) * torch.exp(z_log_std)\n",
    "            features = z_sampled\n",
    "            features_list.append(features)\n",
    "\n",
    "            # Decode the sampled latent variables to generate a new adjacency matrix\n",
    "            new_adj_rec = torch.sigmoid(torch.matmul(z_sampled, z_sampled.t()))\n",
    "            # Generate binary adjacency matrix (threshold the probabilities)\n",
    "            threshold = 0.5  # Adjust the threshold as needed\n",
    "            new_adj_binary = (new_adj_rec > threshold).float()\n",
    "\n",
    "\n",
    "            # convert to a tensor\n",
    "            new_adj_sparse = sp.coo_matrix(new_adj_binary.detach().cpu().numpy())\n",
    "            # Ensure adj is in sparse format after edge masking\n",
    "\n",
    "            # Step 1: Convert sparse matrix to dense format\n",
    "            new_adj_dense = new_adj_sparse.toarray()  # Convert sparse COO matrix to dense array\n",
    "\n",
    "            # Step 2: Convert to PyTorch tensor\n",
    "            new_adj_tensor = torch.tensor(new_adj_dense, dtype=torch.float32, device=self.device)\n",
    "\n",
    "            # Step 3: Extract upper triangular values (excluding diagonal)\n",
    "            idx = torch.triu_indices(new_adj_tensor.size(0), new_adj_tensor.size(1), offset=1)\n",
    "            upper_tri_values = new_adj_tensor[idx[0], idx[1]]  # Extract upper triangular values\n",
    "            \n",
    "            upper_tri_values_list.append(upper_tri_values)\n",
    "\n",
    "            # Step 4: Assign to self.adj_hidden\n",
    "            if upper_tri_values.shape[0] != self.adj_hidden.shape[1]:\n",
    "                raise ValueError(f\"Size mismatch: upper_tri_values has {upper_tri_values.shape} shape, but self.adj_hidden expects {self.adj_hidden.shape}.\")\n",
    "\n",
    "        all_upper_tri_values = torch.stack(upper_tri_values_list, dim=0)  # Concatenate along the first dimension\n",
    "        all_features = torch.stack(features_list, dim=0)  # Concatenate along the first dimension\n",
    "        return all_upper_tri_values, all_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-val iter: 01 epoch: 010 train_loss= 0.71808 train_acc= 0.43457 val_loss= 0.71198 val_acc= 0.57778 time= 6.38957\n",
      "Cross-val iter: 01 epoch: 020 train_loss= 0.67188 train_acc= 0.61111 val_loss= 0.75057 val_acc= 0.56667 time= 5.54245\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in kf.split(y):\n",
    "    it += 1\n",
    "    \n",
    "    idx = np.random.permutation(train_index)\n",
    "    train_index = idx[:int(idx.size*0.9)].tolist()\n",
    "    val_index = idx[int(idx.size*0.9):].tolist()\n",
    "\n",
    "    n_train = len(train_index)\n",
    "    n_val = len(val_index)\n",
    "    n_test = len(test_index)\n",
    "\n",
    "    adj_train = [adj_lst[i] for i in train_index]\n",
    "    features_train = [features_lst[i] for i in train_index]\n",
    "    y_train = [y[i] for i in train_index]\n",
    "\n",
    "    adj_val = [adj_lst[i] for i in val_index]\n",
    "    features_val = [features_lst[i] for i in val_index]\n",
    "    y_val = [y[i] for i in val_index]\n",
    "\n",
    "    adj_test = [adj_lst[i] for i in test_index]\n",
    "    features_test = [features_lst[i] for i in test_index]\n",
    "    y_test = [y[i] for i in test_index]\n",
    "\n",
    "    adj_train, features_train, graph_indicator_train, y_train = generate_batches(adj_train, features_train, y_train, args.batch_size, device)\n",
    "    adj_val, features_val, graph_indicator_val, y_val = generate_batches(adj_val, features_val, y_val, args.batch_size, device)\n",
    "    adj_test, features_test, graph_indicator_test, y_test = generate_batches(adj_test, features_test, y_test, args.batch_size, device)\n",
    "\n",
    "    n_train_batches = ceil(n_train/args.batch_size)\n",
    "    n_val_batches = ceil(n_val/args.batch_size)\n",
    "    n_test_batches = ceil(n_test/args.batch_size)\n",
    "    \n",
    "    model = RW_NN(features_dim, args.max_step, args.hidden_graphs, args.size_hidden_graphs, args.hidden_dim, args.penultimate_dim, args.normalize, n_classes, args.dropout, device).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)\n",
    "\n",
    "    def train(epoch, adj, features, graph_indicator, y):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(adj, features, graph_indicator)\n",
    "        loss_train = F.cross_entropy(output, y)\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "        return output, loss_train\n",
    "\n",
    "    def test(adj, features, graph_indicator, y):\n",
    "        output = model(adj, features, graph_indicator)\n",
    "        loss_test = F.cross_entropy(output, y)\n",
    "        return output, loss_test\n",
    "\n",
    "    best_acc = 0\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        start = time.time()\n",
    "        model.train()\n",
    "        train_loss = AverageMeter()\n",
    "        train_acc = AverageMeter()\n",
    "\n",
    "        # Train for one epoch\n",
    "        for i in range(n_train_batches):\n",
    "            output, loss = train(epoch, adj_train[i], features_train[i], graph_indicator_train[i], y_train[i])\n",
    "            train_loss.update(loss.item(), output.size(0))\n",
    "            train_acc.update(accuracy(output.data, y_train[i].data), output.size(0))\n",
    "\n",
    "        # Evaluate on validation set every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0 or epoch == args.epochs - 1:  # Always evaluate at the last epoch\n",
    "            model.eval()\n",
    "            val_loss = AverageMeter()\n",
    "            val_acc = AverageMeter()\n",
    "\n",
    "            for i in range(n_val_batches):\n",
    "                output, loss = test(adj_val[i], features_val[i], graph_indicator_val[i], y_val[i])\n",
    "                val_loss.update(loss.item(), output.size(0))\n",
    "                val_acc.update(accuracy(output.data, y_val[i].data), output.size(0))\n",
    "\n",
    "            # Print validation results\n",
    "            print(\"Cross-val iter:\", '%02d' % it, \"epoch:\", '%03d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(train_loss.avg),\n",
    "                \"train_acc=\", \"{:.5f}\".format(train_acc.avg), \"val_loss=\", \"{:.5f}\".format(val_loss.avg),\n",
    "                \"val_acc=\", \"{:.5f}\".format(val_acc.avg), \"time=\", \"{:.5f}\".format(time.time() - start))\n",
    "\n",
    "            # Remember best accuracy and save checkpoint\n",
    "            is_best = val_acc.avg >= best_acc\n",
    "            best_acc = max(val_acc.avg, best_acc)\n",
    "            if is_best:\n",
    "                early_stopping_counter = 0\n",
    "                torch.save({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'state_dict': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                }, 'model_best.pth.tar')\n",
    "\n",
    "        # Adjust learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "    print(\"Optimization finished!\")\n",
    "\n",
    "    # Testing\n",
    "    test_loss = AverageMeter()\n",
    "    test_acc = AverageMeter()\n",
    "    print(\"Loading checkpoint!\")\n",
    "    checkpoint = torch.load('model_best.pth.tar')\n",
    "    epoch = checkpoint['epoch']\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "    for i in range(n_test_batches):\n",
    "        output, loss = test(adj_test[i], features_test[i], graph_indicator_test[i], y_test[i])\n",
    "        test_loss.update(loss.item(), output.size(0))\n",
    "        test_acc.update(accuracy(output.data, y_test[i].data), output.size(0))\n",
    "    accs.append(test_acc.avg.cpu().numpy())\n",
    "\n",
    "    # Print results\n",
    "    print(\"test_loss=\", \"{:.5f}\".format(test_loss.avg), \"test_acc=\", \"{:.5f}\".format(test_acc.avg))\n",
    "    print()\n",
    "\n",
    "    print(\"avg_test_acc=\", \"{:.5f}\".format(np.mean(accs)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Graph 0:\n",
      "  Node 0: [1, 2, 3, 4]\n",
      "  Node 1: [0, 2, 3]\n",
      "  Node 2: [0, 1, 3]\n",
      "  Node 3: [0, 1, 2, 4]\n",
      "  Node 4: [0, 3]\n",
      "Hidden Graph 1:\n",
      "  Node 0: [2, 3]\n",
      "  Node 1: []\n",
      "  Node 2: [0, 4]\n",
      "  Node 3: [0]\n",
      "  Node 4: [2]\n",
      "Hidden Graph 2:\n",
      "  Node 0: [1, 2, 3, 4]\n",
      "  Node 1: [0, 3, 4]\n",
      "  Node 2: [0]\n",
      "  Node 3: [0, 1]\n",
      "  Node 4: [0, 1]\n",
      "Hidden Graph 3:\n",
      "  Node 0: [1]\n",
      "  Node 1: [0, 3, 4]\n",
      "  Node 2: [3]\n",
      "  Node 3: [1, 2, 4]\n",
      "  Node 4: [1, 3]\n",
      "Hidden Graph 4:\n",
      "  Node 0: [1, 3]\n",
      "  Node 1: [0]\n",
      "  Node 2: [4]\n",
      "  Node 3: [0, 4]\n",
      "  Node 4: [2, 3]\n",
      "Hidden Graph 5:\n",
      "  Node 0: [1, 2, 4]\n",
      "  Node 1: [0, 3]\n",
      "  Node 2: [0]\n",
      "  Node 3: [1, 4]\n",
      "  Node 4: [0, 3]\n",
      "Hidden Graph 6:\n",
      "  Node 0: [3, 4]\n",
      "  Node 1: [2, 4]\n",
      "  Node 2: [1]\n",
      "  Node 3: [0, 4]\n",
      "  Node 4: [0, 1, 3]\n",
      "Hidden Graph 7:\n",
      "  Node 0: [1, 2, 3]\n",
      "  Node 1: [0, 3, 4]\n",
      "  Node 2: [0, 4]\n",
      "  Node 3: [0, 1]\n",
      "  Node 4: [1, 2]\n"
     ]
    }
   ],
   "source": [
    "adjacency_lists = model.get_hidden_graphs_adjacency_list()\n",
    "\n",
    "# Print the adjacency lists for each hidden graph\n",
    "for i, adj_list in enumerate(adjacency_lists):\n",
    "    print(f\"Hidden Graph {i}:\")\n",
    "    for node, neighbors in adj_list.items():\n",
    "        print(f\"  Node {node}: {neighbors}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "36venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
